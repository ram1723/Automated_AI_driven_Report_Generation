{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ToGWj-l5_JF4"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_community langchain_core tavily-python langchain_nvidia_ai_endpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKUDJbTEOQiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7061c4f9"
      },
      "source": [
        "To use the NVIDIA AI endpoints, you'll need an API key. If you don't already have one, create a key in the NVIDIA AI playground.\n",
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `NVIDIA_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee39b629"
      },
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('nvidia_api_key')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\",temperature=0.2,top_p=0.7)"
      ],
      "metadata": {
        "id": "KWJfJMXCPE2N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "client = ChatNVIDIA(\n",
        "  model=\"meta/llama-3.3-70b-instruct\",\n",
        "  api_key=userdata.get('nvidia_api_key'),\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_tokens=1024,\n",
        ")\n",
        "\n",
        "for chunk in client.stream([{\"role\":\"user\",\"content\":\"Write a ballad about LangChain.\"}]):\n",
        "  print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OhNueS_3OwPE",
        "outputId": "653a109b-9aee-40e8-ec7a-f7fd5a6bf513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2835055790.py:3: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
            "  client = ChatNVIDIA(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Verse 1)\n",
            "In realms of code, where innovators roam\n",
            "A new legend emerged, to make the future home\n",
            "LangChain, a name that echoes through the land\n",
            "A framework born, to harness AI's command\n",
            "\n",
            "(Chorus)\n",
            "Oh LangChain, oh LangChain, a chain of might\n",
            "Linking languages, in a wondrous sight\n",
            "From Python to JavaScript, and all in between\n",
            "A bridge of code, where AI's secrets are seen\n",
            "\n",
            "(Verse 2)\n",
            "With LLMs as guides, and a modular design\n",
            "LangChain weaves a tapestry, of interoperable lines\n",
            "A hub of creativity, where developers thrive\n",
            "Building applications, that touch the sky\n",
            "\n",
            "(Chorus)\n",
            "Oh LangChain, oh LangChain, a chain of might\n",
            "Linking languages, in a wondrous sight\n",
            "From Python to JavaScript, and all in between\n",
            "A bridge of code, where AI's secrets are seen\n",
            "\n",
            "(Verse 3)\n",
            "In the realm of NLP, where words are the key\n",
            "LangChain unlocks the door, to a world of possibility\n",
            "Text classification, generation, and more\n",
            "The boundaries of language, are pushed to the shore\n",
            "\n",
            "(Chorus)\n",
            "Oh LangChain, oh LangChain, a chain of might\n",
            "Linking languages, in a wondrous sight\n",
            "From Python to JavaScript, and all in between\n",
            "A bridge of code, where AI's secrets are seen\n",
            "\n",
            "(Bridge)\n",
            "As the chain grows stronger, with each passing day\n",
            "The future of AI, begins to find its way\n",
            "Through the contributions, of a community so grand\n",
            "LangChain's potential, will forever"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2835055790.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Write a ballad about LangChain.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minput_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LC_ID_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                         \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/chat_models.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         ):\n\u001b[1;32m    448\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         for response in self._client.get_req_stream(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py\u001b[0m in \u001b[0;36mout_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mout_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;31m## Good for client, since it allows self.last_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"data: [DONE]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36miter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mpending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         for chunk in self.iter_content(\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \"\"\"\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('tavily_api_key')"
      ],
      "metadata": {
        "id": "GdMAi2C4SeJD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient, AsyncTavilyClient\n",
        "tavily_client = TavilyClient()\n",
        "tavily_async_client = AsyncTavilyClient()"
      ],
      "metadata": {
        "id": "Tuws0zCRO6P5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('langchain_api_key')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"report-mAIstro\""
      ],
      "metadata": {
        "id": "gwCQ3BOLRtk7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report Plan, utils functions"
      ],
      "metadata": {
        "id": "fiW-La4DPt_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from langsmith import traceable\n",
        "from pydantic import BaseModel, Field\n",
        "class Section(BaseModel):\n",
        "    name: str = Field(\n",
        "        description=\"Name for this section of the report.\",\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
        "    )\n",
        "    research: bool = Field(\n",
        "        description=\"Whether to perform web research for this section of the report.\"\n",
        "    )\n",
        "    content: str = Field(\n",
        "        description=\"The content of the section.\"\n",
        "    )\n",
        "\n",
        "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "\n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response['results']\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and 'results' in response:\n",
        "                sources_list.extend(response['results'])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
        "\n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source['url'] not in unique_sources:\n",
        "            unique_sources[source['url']] = source\n",
        "\n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get('raw_content', '')\n",
        "            if raw_content is None:\n",
        "                raw_content = ''\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "\n",
        "    return formatted_text.strip()\n",
        "def format_sections(sections: list[Section]) -> str:\n",
        "    \"\"\" Format a list of sections into a string \"\"\"\n",
        "    formatted_str = \"\"\n",
        "    for idx, section in enumerate(sections, 1):\n",
        "        formatted_str += f\"\"\"\n",
        "          Section {idx}: {section.name}\n",
        "          Description:\n",
        "          {section.description}\n",
        "          Requires Research:\n",
        "          {section.research}\n",
        "\n",
        "          Content:\n",
        "          {section.content if section.content else '[Not yet written]'}\n",
        "\n",
        "          \"\"\"\n",
        "    return formatted_str\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query):\n",
        "    \"\"\" Search the web using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "\n",
        "    Returns:\n",
        "        dict: Tavily search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "\n",
        "    return tavily_client.search(query,\n",
        "                         max_results=5,\n",
        "                         include_raw_content=True)\n",
        "\n",
        "@traceable\n",
        "async def tavily_search_async(search_queries, tavily_topic, tavily_days):\n",
        "    \"\"\"\n",
        "    Performs concurrent web searches using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        search_queries (List[SearchQuery]): List of search queries to process\n",
        "        tavily_topic (str): Type of search to perform ('news' or 'general')\n",
        "        tavily_days (int): Number of days to look back for news articles (only used when tavily_topic='news')\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: List of search results from Tavily API, one per query\n",
        "\n",
        "    Note:\n",
        "        For news searches, each result will include articles from the last `tavily_days` days.\n",
        "        For general searches, the time range is unrestricted.\n",
        "    \"\"\"\n",
        "\n",
        "    search_tasks = []\n",
        "    for query in search_queries:\n",
        "        if tavily_topic == \"news\":\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"news\",\n",
        "                    days=tavily_days\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"general\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Execute all searches concurrently\n",
        "    search_docs = await asyncio.gather(*search_tasks)\n",
        "\n",
        "    return search_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "DsETMFFoPJQ9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Planning"
      ],
      "metadata": {
        "id": "uGkKkyIhUl6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import  Annotated, List, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "class Sections(BaseModel):\n",
        "    sections: List[Section] = Field(\n",
        "        description=\"Sections of the report.\",\n",
        "    )\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(\n",
        "        None, description=\"Query for web search.\"\n",
        "    )\n",
        "class Queries(BaseModel):\n",
        "    queries: List[SearchQuery] = Field(\n",
        "        description=\"List of search queries.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "uP5Tf2NCV-2G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "class ReportState(TypedDict):\n",
        "  topic: str\n",
        "  tavily_topic: Literal[\"general\", \"news\"]\n",
        "  tavily_days: Optional[int]\n",
        "  report_structure: str\n",
        "  number_of_queries: int\n",
        "  sections: List[Section]\n",
        "  completed_sections: Annotated[list, operator.add]\n",
        "  report_sections_from_research: str\n",
        "  final_report: str"
      ],
      "metadata": {
        "id": "JSYFKLWSU0dk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "report_planner_query_writer_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "The report will be focused on the following topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report structure will follow these guidelines:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information for planning the report sections.\n",
        "\n",
        "The query should:\n",
        "\n",
        "1. Be related to the topic\n",
        "2. Help satisfy the requirements specified in the report organization\n",
        "\n",
        "Make the query specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\"\"\"\n",
        "\n",
        "report_planner_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "Your goal is to generate the outline of the sections of the report.\n",
        "\n",
        "The overall topic of the report is:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report should follow this organization:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "You should reflect on this information to plan the sections of the report:\n",
        "\n",
        "{context}\n",
        "\n",
        "Now, generate the sections of the report. Each section should have the following fields:\n",
        "\n",
        "- Name - Name for this section of the report.\n",
        "- Description - Brief overview of the main topics and concepts to be covered in this section.\n",
        "- Research - Whether to perform web research for this section of the report.\n",
        "- Content - The content of the section, which you will leave blank for now.\n",
        "\n",
        "Consider which sections require web research. For example, introduction and conclusion will not require research because they will distill information from other parts of the report.\"\"\"\n",
        "\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "\n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "def invoke_structured_llm_with_retry(structured_llm, queries, max_attempts=3):\n",
        "    for _ in range(max_attempts):\n",
        "        results = structured_llm.invoke(queries)\n",
        "        if results:\n",
        "            return results\n",
        "    return results\n",
        "\n",
        "async def generate_report_plan(state: ReportState):\n",
        "\n",
        "    topic = state[\"topic\"]\n",
        "    report_structure = state[\"report_structure\"]\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
        "    results = invoke_structured_llm_with_retry(structured_llm,[SystemMessage(content=system_instructions_query)]+[HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
        "    query_list = [query.search_query for query in results.queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=1000, include_raw_content=True)\n",
        "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str)\n",
        "    structured_llm = llm.with_structured_output(Sections)\n",
        "    report_sections = invoke_structured_llm_with_retry(structured_llm,[SystemMessage(content=system_instructions_sections)]+[HumanMessage(content=\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. Each section must have: name, description, plan, research, and content fields.\")])\n",
        "    return {\"sections\": report_sections.sections}"
      ],
      "metadata": {
        "id": "ybmYJn4tU6_1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_topic = \"Give an overview of capabilities and specific use case examples for these processing units: CPU, GPU.\"\n",
        "tavily_topic = \"general\"\n",
        "tavily_days = None # Only applicable for news topic\n",
        "\n",
        "# Generate report plan\n",
        "sections = await generate_report_plan({\"topic\": report_topic, \"report_structure\": report_structure, \"number_of_queries\": 2, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})\n",
        "\n",
        "# Print sections\n",
        "for section in sections['sections']:\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Name: {section.name}\")\n",
        "    print(f\"Description: {section.description}\")\n",
        "    print(f\"Research: {section.research}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGtTVpTHOnfW",
        "outputId": "c33fa5b7-18f8-4f80-9695-6c9e8ede5f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No raw_content found for source https://www.reddit.com/r/computerscience/comments/t3nltj/eli5_what_is_the_difference_between_a_gpu_and_a/\n",
            "==================================================\n",
            "Name: Introduction\n",
            "Description: Provide a brief overview of the topic area and context for the comparison between CPU and GPU.\n",
            "Research: False\n",
            "==================================================\n",
            "Name: CPU\n",
            "Description: Examine the core features, architecture, and implementation of CPU, and provide an example use case.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: GPU\n",
            "Description: Examine the core features, architecture, and implementation of GPU, and provide an example use case.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: Conclusion with Comparison Table\n",
            "Description: Present a structured comparison table that highlights the relative strengths and weaknesses of CPU and GPU, and provide final recommendations.\n",
            "Research: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SectionState(TypedDict):\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    number_of_queries: int # Number web search queries to perform per section\n",
        "    section: Section # Report section\n",
        "    search_queries: list[SearchQuery] # List of search queries\n",
        "    source_str: str # String of formatted source content from web search\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "class SectionOutputState(TypedDict):\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Query writer instructions\n",
        "query_writer_instructions=\"\"\"Your goal is to generate targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "When generating {number_of_queries} search queries, ensure they:\n",
        "1. Cover different aspects of the topic (e.g., core features, real-world applications, technical architecture)\n",
        "2. Include specific technical terms related to the topic\n",
        "3. Target recent information by including year markers where relevant (e.g., \"2024\")\n",
        "4. Look for comparisons or differentiators from similar technologies/approaches\n",
        "5. Search for both official documentation and practical implementation examples\n",
        "\n",
        "Your queries should be:\n",
        "- Specific enough to avoid generic results\n",
        "- Technical enough to capture detailed implementation information\n",
        "- Diverse enough to cover all aspects of the section plan\n",
        "- Focused on authoritative sources (documentation, technical blogs, academic papers)\"\"\"\n",
        "\n",
        "# Section writer instructions\n",
        "section_writer_instructions = \"\"\"You are an expert technical writer crafting one section of a technical report.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "Guidelines for writing:\n",
        "\n",
        "1. Technical Accuracy:\n",
        "- Include specific version numbers\n",
        "- Reference concrete metrics/benchmarks\n",
        "- Cite official documentation\n",
        "- Use technical terminology precisely\n",
        "\n",
        "2. Length and Style:\n",
        "- Strict 150-200 word limit\n",
        "- No marketing language\n",
        "- Technical focus\n",
        "- Write in simple, clear language\n",
        "- Start with your most important insight in **bold**\n",
        "- Use short paragraphs (2-3 sentences max)\n",
        "\n",
        "3. Structure:\n",
        "- Use ## for section title (Markdown format)\n",
        "- Only use ONE structural element IF it helps clarify your point:\n",
        "  * Either a focused table comparing 2-3 key items (using Markdown table syntax)\n",
        "  * Or a short list (3-5 items) using proper Markdown list syntax:\n",
        "    - Use `*` or `-` for unordered lists\n",
        "    - Use `1.` for ordered lists\n",
        "    - Ensure proper indentation and spacing\n",
        "- End with ### Sources that references the below source material formatted as:\n",
        "  * List each source with title, date, and URL\n",
        "  * Format: `- Title : URL`\n",
        "\n",
        "3. Writing Approach:\n",
        "- Include at least one specific example or case study\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- No preamble prior to creating the section content\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Use this source material to help write the section:\n",
        "{context}\n",
        "\n",
        "5. Quality Checks:\n",
        "- Exactly 150-200 words (excluding title and sources)\n",
        "- Careful use of only ONE structural element (table or list) and only if it helps clarify your point\n",
        "- One specific example / case study\n",
        "- Starts with bold insight\n",
        "- No preamble prior to creating the section content\n",
        "- Sources cited at end\"\"\"\n",
        "\n",
        "def generate_queries(state: SectionState):\n",
        "    \"\"\" Generate search queries for a section \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    section = state[\"section\"]\n",
        "\n",
        "    # Generate queries\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = query_writer_instructions.format(section_topic=section.description, number_of_queries=number_of_queries)\n",
        "\n",
        "    # Generate queries\n",
        "    queries = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
        "\n",
        "    return {\"search_queries\": queries.queries}\n",
        "\n",
        "async def search_web(state: SectionState):\n",
        "    \"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n",
        "\n",
        "    # Get state\n",
        "    search_queries = state[\"search_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in search_queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n",
        "\n",
        "    return {\"source_str\": source_str}\n",
        "\n",
        "def write_section(state: SectionState):\n",
        "    \"\"\" Write a section of the report \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    section = state[\"section\"]\n",
        "    source_str = state[\"source_str\"]\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=source_str)\n",
        "\n",
        "    # Generate section\n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "\n",
        "    # Write content to the section object\n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "# Add nodes and edges\n",
        "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
        "section_builder.add_node(\"generate_queries\", generate_queries)\n",
        "section_builder.add_node(\"search_web\", search_web)\n",
        "section_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "section_builder.add_edge(START, \"generate_queries\")\n",
        "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
        "section_builder.add_edge(\"search_web\", \"write_section\")\n",
        "section_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Compile\n",
        "section_builder_graph = section_builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(section_builder_graph.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Ch3qerY8O7wc",
        "outputId": "0f6c1cde-fbb7-4a47-918c-e1e870310232"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2562119452.py:144: LangGraphDeprecatedSinceV05: `output` is deprecated and will be removed. Please use `output_schema` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
            "  section_builder = StateGraph(SectionState, output=SectionOutputState)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with one section\n",
        "sections = sections['sections']\n",
        "test_section = sections[1]\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Name: {test_section.name}\")\n",
        "print(f\"Description: {test_section.description}\")\n",
        "print(f\"Research: {test_section.research}\")\n",
        "\n",
        "# Run\n",
        "report_section = await section_builder_graph.ainvoke({\"section\": test_section, \"number_of_queries\": 2, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})\n",
        "from IPython.display import Markdown\n",
        "section = report_section['completed_sections'][0]\n",
        "Markdown(section.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "H7pzN1FHQf6o",
        "outputId": "e82b2087-e55d-4c31-d170-407cac2149ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Name: CPU\n",
            "Description: Examine the core features, architecture, and implementation of CPU, and provide an example use case.\n",
            "Research: True\n",
            "Warning: No raw_content found for source https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-a-profile-architecture-developments-2024\n",
            "Warning: No raw_content found for source https://www.reddit.com/r/ECE/comments/1cygncn/whats_the_best_processor_architecture_currently/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## CPU Architecture and Performance\n**The CPU market is highly competitive, with AMD and Intel offering high-performance processors**. The AMD Ryzen 9 7950X3D and Intel Core i9-14900K are top contenders, with the Ryzen 9 7950X3D offering stronger gaming performance. The CPU market is expected to see significant changes with the introduction of new architectures, such as AMD's Zen 5 and Intel's Arrow Lake.\n\nSome key features of modern CPU architectures include:\n* Improved IPC (Instructions Per Cycle)\n* Higher clock speeds\n* Increased core counts\n* Enhanced security features\n\nFor example, the AMD Ryzen 7 9800X3D offers 8 cores and 16 threads, with a 4.7 GHz base clock and 5.2 GHz boost clock. This processor is well-suited for gaming and content creation workloads.\n\n### Sources\n- Best CPUs for Gaming: July 2024: https://www.anandtech.com/show/9793/best-cpus\n- Which CPU brand is more popular in 2024? (Consumer grade desktop): https://www.reddit.com/r/buildapc/comments/1gilps1/which_cpu_brand_is_more_popular_in_2024_consumer/\n- Best CPUs of 2024 (Intel vs. AMD): Gaming, Production, Budget, & Efficiency: https://www.tomshardware.com/reviews/best-cpus,3986.html\n- The Best CPU for Gaming in 2025: https://www.tomshardware.com/reviews/best-cpus-for-gaming,3986.html\n- CPU Benchmarks and Hierarchy 2025: CPU Rankings: https://www.tomshardware.com/reviews/cpu-hierarchy,4312.html\n- CPU Architecture DEEP DIVE: https://pocketmags.com/us/maximum-pc-magazine/october-2024/articles/cpu-architecture-deep-dive?srsltid=AfmBOoo5UWqn7Jdfq6hys1boFelrXQ3C8Sxc_yViC66_4C4c8612Swk7\n- Arm A-Profile Architecture Developments 2024: https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-a-profile-architecture-developments-2024\n- What's the best processor architecture currently, from a technical standpoint?: https://www.reddit.com/r/ECE/comments/1cygncn/whats_the_best_processor_architecture_currently/\n- CPU Examples, Applications and Use Cases: https://www.ibm.com/think/topics/cpu-use-cases\n- 5th Gen AMD EPYC Processor Architecture: https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/5th-gen-amd-epyc-processor-architecture-white-paper.pdf"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReportStateOutput(TypedDict):\n",
        "    final_report: str # Final report\n",
        "\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "\n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "# Tavily search parameters\n",
        "tavily_topic = \"general\"\n",
        "tavily_days = None # Only applicable for news topic\n",
        "\n",
        "from langgraph.constants import Send\n",
        "\n",
        "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
        "\n",
        "Section to write:\n",
        "{section_topic}\n",
        "\n",
        "Available report content:\n",
        "{context}\n",
        "\n",
        "1. Section-Specific Approach:\n",
        "\n",
        "For Introduction:\n",
        "- Use # for report title (Markdown format)\n",
        "- 50-100 word limit\n",
        "- Write in simple and clear language\n",
        "- Focus on the core motivation for the report in 1-2 paragraphs\n",
        "- Use a clear narrative arc to introduce the report\n",
        "- Include NO structural elements (no lists or tables)\n",
        "- No sources section needed\n",
        "\n",
        "For Conclusion/Summary:\n",
        "- Use ## for section title (Markdown format)\n",
        "- 100-150 word limit\n",
        "- For comparative reports:\n",
        "    * Must include a focused comparison table using Markdown table syntax\n",
        "    * Table should distill insights from the report\n",
        "    * Keep table entries clear and concise\n",
        "- For non-comparative reports:\n",
        "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
        "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
        "    * Or a short list using proper Markdown list syntax:\n",
        "      - Use `*` or `-` for unordered lists\n",
        "      - Use `1.` for ordered lists\n",
        "      - Ensure proper indentation and spacing\n",
        "- End with specific next steps or implications\n",
        "- No sources section needed\n",
        "\n",
        "3. Writing Approach:\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Quality Checks:\n",
        "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
        "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
        "- Markdown format\n",
        "- Do not include word count or any preamble in your response\"\"\"\n",
        "\n",
        "def initiate_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"\n",
        "\n",
        "    # Kick off section writing in parallel via Send() API for any sections that require research\n",
        "    return [\n",
        "        Send(\"build_section_with_web_research\", {\"section\": s,\n",
        "                                                 \"number_of_queries\": state[\"number_of_queries\"],\n",
        "                                                 \"tavily_topic\": state[\"tavily_topic\"],\n",
        "                                                 \"tavily_days\": state.get(\"tavily_days\", None)})\n",
        "        for s in state[\"sections\"]\n",
        "        if s.research\n",
        "    ]\n",
        "\n",
        "def write_final_sections(state: SectionState):\n",
        "    \"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    section = state[\"section\"]\n",
        "    completed_report_sections = state[\"report_sections_from_research\"]\n",
        "\n",
        "    system_instructions = final_section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=completed_report_sections)\n",
        "\n",
        "    # Generate section\n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    section.content = section_content.content\n",
        "\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "def gather_completed_sections(state: ReportState):\n",
        "    \"\"\" Gather completed sections from research \"\"\"\n",
        "\n",
        "    # List of completed sections\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    # Format completed section to str to use as context for final sections\n",
        "    completed_report_sections = format_sections(completed_sections)\n",
        "\n",
        "    return {\"report_sections_from_research\": completed_report_sections}\n",
        "\n",
        "def initiate_final_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off research on any sections that require it using the Send API \"\"\"\n",
        "\n",
        "    # for any sections that do not require research\n",
        "    return [\n",
        "        Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]})\n",
        "        for s in state[\"sections\"]\n",
        "        if not s.research\n",
        "    ]\n",
        "\n",
        "def compile_final_report(state: ReportState):\n",
        "    \"\"\" Compile the final report \"\"\"\n",
        "\n",
        "    # Get sections\n",
        "    sections = state[\"sections\"]\n",
        "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
        "\n",
        "    # Update sections with completed content while maintaining original order\n",
        "    for section in sections:\n",
        "        section.content = completed_sections[section.name]\n",
        "\n",
        "    # Compile final report\n",
        "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
        "\n",
        "    return {\"final_report\": all_sections}\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(ReportState, output=ReportStateOutput)\n",
        "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
        "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
        "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
        "builder.add_node(\"write_final_sections\", write_final_sections)\n",
        "builder.add_node(\"compile_final_report\", compile_final_report)\n",
        "builder.add_edge(START, \"generate_report_plan\")\n",
        "builder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\n",
        "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
        "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
        "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
        "builder.add_edge(\"compile_final_report\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "ClS1I5h7QshA",
        "outputId": "c1759dc8-5ae3-4c93-bdde-c49d604804a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2509919683.py:30: LangGraphDeprecatedSinceV10: Importing Send from langgraph.constants is deprecated. Please use 'from langgraph.types import Send' instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  from langgraph.constants import Send\n",
            "/tmp/ipython-input-2509919683.py:145: LangGraphDeprecatedSinceV05: `output` is deprecated and will be removed. Please use `output_schema` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
            "  builder = StateGraph(ReportState, output=ReportStateOutput)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate podcast or can be Added as an edge in the graph to create a podcast passing the final report"
      ],
      "metadata": {
        "id": "QfbRtP8P1hRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"google-genai>=1.16.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjKknlUO4rj_",
        "outputId": "d30dd7c3-1582-4708-da95-96c7372f7bc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/222.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m215.0/222.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.6/222.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "SgGbchdE4tnc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "hF80efeP5KvY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import wave\n",
        "\n",
        "# Set up the wave file to save the output:\n",
        "def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
        "   with wave.open(filename, \"wb\") as wf:\n",
        "      wf.setnchannels(channels)\n",
        "      wf.setsampwidth(sample_width)\n",
        "      wf.setframerate(rate)\n",
        "      wf.writeframes(pcm)\n",
        "def create_podcast_discussion(final_report, topic, filename=\"research_podcast.wav\", configuration=None):\n",
        "      script_prompt = f\"\"\"\n",
        "    Create a natural, engaging podcast conversation between Dr. Sarah (research expert) and Mike (curious interviewer) about \"{topic}\".\n",
        "\n",
        "    Use this research content:\n",
        "\n",
        "  FINAL REPORT GENERATED BASED ON THE WEBSEARCH AND RESEARCH RESULTS:\n",
        "  {final_report}\n",
        "\n",
        "    Format as a dialogue with:\n",
        "    - Mike introducing the topic and asking questions\n",
        "    - Dr. Sarah explaining key concepts and insights\n",
        "    - Natural back-and-forth discussion (5-7 exchanges)\n",
        "    - Mike asking follow-up questions\n",
        "    - Dr. Sarah synthesizing the main takeaways\n",
        "    - Keep it conversational and accessible (2 or 3-4 minutes when spoken)\n",
        "\n",
        "    Format exactly like this:\n",
        "    Mike: [opening question]\n",
        "    Dr. Sarah: [expert response]\n",
        "    Mike: [follow-up]\n",
        "    Dr. Sarah: [explanation]\n",
        "    [continue...]\n",
        "    \"\"\"\n",
        "      script_response = client.models.generate_content(\n",
        "          model=\"gemini-2.5-flash\",\n",
        "          contents=script_prompt\n",
        "      )\n",
        "      podcast_script = script_response.candidates[0].content.parts[0].text\n",
        "      tts_prompt = f\"TTS the following conversation between Mike and Dr. Sarah:\\n{podcast_script}\"\n",
        "\n",
        "      # Use default values if configuration is None\n",
        "      tts_model = getattr(configuration, 'tts_model', \"gemini-2.5-flash-preview-tts\")\n",
        "      mike_voice = getattr(configuration, 'mike_voice', 'Kore')\n",
        "      sarah_voice = getattr(configuration, 'sarah_voice', 'Puck')\n",
        "      tts_channels = getattr(configuration, 'tts_channels', 1)\n",
        "      tts_rate = getattr(configuration, 'tts_rate', 24000)\n",
        "      tts_sample_width = getattr(configuration, 'tts_sample_width', 2)\n",
        "\n",
        "\n",
        "      response = client.models.generate_content(\n",
        "          model=tts_model,\n",
        "          contents=tts_prompt,\n",
        "          config=types.GenerateContentConfig(\n",
        "              response_modalities=[\"AUDIO\"],\n",
        "              speech_config=types.SpeechConfig(\n",
        "                  multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n",
        "                      speaker_voice_configs=[\n",
        "                          types.SpeakerVoiceConfig(\n",
        "                              speaker='Mike',\n",
        "                              voice_config=types.VoiceConfig(\n",
        "                                  prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                      voice_name=mike_voice,\n",
        "                                  )\n",
        "                              )\n",
        "                          ),\n",
        "                          types.SpeakerVoiceConfig(\n",
        "                              speaker='Dr. Sarah',\n",
        "                              voice_config=types.VoiceConfig(\n",
        "                                  prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                      voice_name=sarah_voice,\n",
        "                                  )\n",
        "                                )\n",
        "                            ),\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "      audio_data = response.candidates[0].content.parts[0].inline_data.data\n",
        "      wave_file(filename, audio_data, tts_channels, tts_rate, tts_sample_width)\n",
        "\n",
        "      print(f\"Podcast saved as: {filename}\")\n",
        "      return podcast_script, filename\n",
        "\n",
        "# # Add nodes and edges\n",
        "# builder = StateGraph(ReportState, output=ReportStateOutput)\n",
        "# builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
        "# builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
        "# builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
        "# builder.add_node(\"write_final_sections\", write_final_sections)\n",
        "# builder.add_node(\"compile_final_report\", compile_final_report)\n",
        "# builder.add_edge(START, \"generate_report_plan\")\n",
        "# builder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\n",
        "# builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
        "# builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
        "# builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
        "# builder.add_edge(\"compile_final_report\", \"create_podcast_discussion\")\n",
        "# builder.add_edge(\"create_podcast_discussion\", END)\n",
        "\n",
        "# graph = builder.compile()\n",
        "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "D9Xjc3LoyDjI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_topic = \"Give an overview of capabilities and specific use case examples for these processing units: CPU, GPU.\"\n",
        "report = await graph.ainvoke({\"topic\": report_topic,\n",
        "                                   \"report_structure\": report_structure,\n",
        "                                   \"number_of_queries\": 2,\n",
        "                                   \"tavily_topic\": tavily_topic,\n",
        "                                   \"tavily_days\": tavily_days})\n",
        "from IPython.display import Markdown\n",
        "Markdown(report['final_report'])\n",
        "#print the final report in plain text\n",
        "report['final_report']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ByexUEtdSEjq",
        "outputId": "aace6c9a-043b-4bb8-a413-0862d14cd09c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No raw_content found for source https://www.intel.com/content/www/us/en/products/docs/processors/cpu-vs-gpu.html\n",
            "Warning: No raw_content found for source https://www.reddit.com/r/apple/comments/1449eiq/what_exactly_are_the_use_cases_for_the_extra_cpu/\n",
            "Warning: No raw_content found for source https://www.cdw.com/content/cdw/en/articles/hardware/cpu-vs-gpu.html\n",
            "Warning: No raw_content found for source https://blogs.nvidia.com/blog/siggraph-2024-ai-graphics-research/\n",
            "Warning: No raw_content found for source https://opencv.org/blog/nvidia-ai-deep-learning-projects/\n",
            "Warning: No raw_content found for source https://www.youtube.com/watch?v=6OFdcli_nGE&pp=ygUHI2NydjMycg%3D%3D\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Introduction to CPU and GPU Comparison\\nThe central processing unit (CPU) and graphics processing unit (GPU) are fundamental components of modern computing systems. Understanding their architectures and implementations is crucial for determining their performance and efficiency in various applications.\\n\\n## Context for Comparison\\nThe CPU, or Central Processing Unit, is the brain of the computer, responsible for executing instructions and handling data. In contrast, the GPU is designed for massive parallel processing tasks, making it ideal for AI applications such as deep learning and computer vision. The comparison between CPU and GPU is essential for selecting the appropriate processing unit for specific use cases, including AI, gaming, and scientific simulations.\\n\\n## CPU Architecture\\n**The core of modern computing, CPU architecture plays a crucial role in determining the performance and efficiency of a system**. The CPU, or Central Processing Unit, is the brain of the computer, responsible for executing instructions and handling data. The architecture of the CPU refers to the design and structure of the processor, including the instruction set, execution units, and memory hierarchy.\\n\\nThe sources provided offer a wealth of information on CPU architecture, including the differences between CPUs and GPUs, the evolution of CPU design, and the latest advancements in CPU technology. For example, the source \"Decoding CPU vs. GPU: A Detailed Exploration of NVIDIA and AMD GPU Architectures\" provides a comprehensive overview of the architectural differences between CPUs and GPUs, highlighting the strengths and weaknesses of each.\\n\\nOne specific example of CPU architecture is the NVIDIA Hopper H100 Tensor Core GPU, which is designed for large-scale AI and HPC workloads. This GPU features a number of innovative technologies, including fourth-generation Tensor Cores, a transformer engine, and a 50MB L2 cache architecture. These features enable the H100 to deliver exceptional performance and efficiency for AI and HPC applications.\\n\\nIn terms of CPU design, the sources highlight the importance of instruction-level parallelism (ILP) and thread-level parallelism (TLP) in achieving high performance. The use of techniques such as superscalar execution, out-of-order execution, and speculative execution can help to improve ILP, while TLP can be achieved through the use of multiple cores and threads.\\n\\nThe sources also discuss the latest advancements in CPU technology, including the use of 3D stacked processors, neuromorphic computing, and quantum computing. These technologies have the potential to revolutionize the field of computing, enabling new applications and use cases that were previously impossible.\\n\\n### Sources\\n- Decoding CPU vs. GPU: A Detailed Exploration of NVIDIA and AMD GPU Architectures: https://medium.com/@rohithreddy66666/decoding-cpu-vs-gpu-a-detailed-exploration-of-nvidia-and-amd-gpu-architectures-e09ebfb594ed\\n- Understanding GPU Architecture: Structure, Layers & Performance Explained: https://www.scalecomputing.com/resources/understanding-gpu-architecture\\n- NVIDIA Hopper Architecture In-Depth: https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\\n- CS8803 OMSCS - GPU hardware and software notes: https://lowyx.com/posts/gt-gpu-notes/\\n- RevaMp3D: Architecting the Processor Core and Cache Hierarchy for Systems with Monolithically-Integrated Logic and Memory: https://arxiv.org/html/2210.08508v2\\n- Arm vs. RISC-V in 2025: Which Architecture Will Lead the Way?: https://www.eetimes.eu/arm-vs-risc-v-in-2025-which-architecture-will-lead-the-way/\\n- What is RISC-V? - Electromaker.io: https://www.electromaker.io/blog/article/what-is-risc-v\\n- RISC-V News: https://riscv.org/about/news/\\n- x86 instruction listings - Wikipedia: https://en.wikipedia.org/wiki/X86_instruction_listings\\n\\n## GPU Architecture and AI Applications\\n**The core feature of modern GPUs is their ability to handle massive parallel processing tasks**, making them ideal for AI applications such as deep learning, natural language processing, and computer vision. The GPU architecture is designed to maximize parallel processing throughput, with thousands of simpler cores working simultaneously to execute instructions on different data points. This is in contrast to CPUs, which are optimized for sequential processing performance and versatility.\\n\\nA specific example of the effectiveness of GPUs in AI applications is the training of large language models like GPT-4, which requires massive computational power and memory bandwidth. GPUs like NVIDIA\\'s H100 series, with their Tensor Cores and high-bandwidth memory, are particularly well-suited for such tasks, offering significant performance improvements over CPUs.\\n\\nKey features of modern GPUs for AI applications include:\\n- High-performance Tensor Cores for accelerated matrix operations\\n- Large memory capacities (up to 80GB) for handling big datasets\\n- Advanced software ecosystems like CUDA and cuDNN for optimized AI performance\\n\\n### Sources\\n- Decoding CPU vs. GPU: A Detailed Exploration of NVIDIA and AMD GPU Architectures: https://medium.com/@rohithreddy66666/decoding-cpu-vs-gpu-a-detailed-exploration-of-nvidia-and-amd-gpu-architectures-e09ebfb594ed\\n- GPU vs CPU for AI: A Detailed Comparison: https://www.trgdatacenters.com/resource/gpu-vs-cpu-for-ai/\\n- From Pixel to Parallel: Understanding Modern GPU Architecture: https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures\\n- The Ultimate Guide to GPUs for Machine Learning in 2025: https://blog.spheron.network/the-ultimate-guide-to-gpus-for-machine-learning-in-2025\\n- The Role of GPUs in Deep Learning and Computational Advancements: https://eastpublication.com/index.php/eje/article/download/34/13\\n- NVIDIA Research Presents AI and Simulation Advancements: https://blogs.nvidia.com/blog/siggraph-2024-ai-graphics-research/\\n- NVIDIA\\'s AI Stack for Deep Learning Projects: https://opencv.org/blog/nvidia-ai-deep-learning-projects/\\n- AI Applications Using GPUs: https://ecosystem.aethir.com/blog-posts/ai-applications-using-gpus-enhancing-computational-efficiency-and-performance\\n\\n# CPU and GPU Comparison\\n## Conclusion and Recommendations\\n\\nThe following comparison table highlights the key differences between CPU and GPU architectures:\\n| Dimension | CPU | GPU |\\n| --- | --- | --- |\\n| Core Features | Optimized for sequential processing | Optimized for parallel processing |\\n| Architecture | Instruction set, execution units, memory hierarchy | Thousands of simpler cores for massive parallel processing |\\n| AI Applications | Less efficient for deep learning, natural language processing | Ideal for deep learning, natural language processing, computer vision |\\n| Performance | High performance for sequential tasks | High performance for parallel tasks |\\n\\nBased on the comparison, GPUs are recommended for AI applications and large-scale parallel processing tasks, while CPUs are suitable for sequential processing and general computing tasks. Next steps include selecting the appropriate architecture based on specific use cases and exploring advancements in CPU and GPU technologies for improved performance and efficiency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_report = report['final_report']\n",
        "create_podcast_discussion(final_report, report_topic, filename=\"research_podcast.wav\", configuration=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owtIOaEn2-NN",
        "outputId": "138a4b4c-c25c-4c01-9a1c-1bc3f524bbf3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Podcast saved as: research_podcast.wav\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Mike: Welcome back to \"Tech Demystified,\" the podcast where we break down complex concepts into everyday language. Today, we\\'re diving into the very heart of computing, something crucial for everything from gaming to AI. Dr. Sarah, so glad to have you here to help us understand the capabilities and use cases for CPUs and GPUs. Let\\'s start with the basics: What\\'s the fundamental difference between a CPU and a GPU?\\n\\nDr. Sarah: Thanks, Mike! It\\'s a great topic because these two components are fundamental. Think of the CPU, the Central Processing Unit, as the *brain* of your computer. Itâ€™s a generalist, incredibly good at executing instructions one after another, handling complex logic, and managing all the different parts of your system. It\\'s responsible for the core operations, from running your operating system to opening your web browser.\\n\\nMike: So, the CPU is the master multi-tasker, the general manager. What about the GPU, the Graphics Processing Unit? We hear a lot about them these days, especially with AI. How does it differ?\\n\\nDr. Sarah: That\\'s where it gets really interesting! While the CPU is a few powerful, versatile cores, the GPU is designed with *thousands* of simpler, smaller cores. This architecture makes it exceptionally good at what we call \"massive parallel processing.\" Instead of doing one complex task very fast, it can do thousands of simpler tasks *simultaneously*. This fundamental difference makes it ideal for things like rendering graphics, scientific simulations, and yes, most notably, AI applications.\\n\\nMike: Ah, \"massive parallel processing\" â€“ that makes sense for things like graphics where you\\'re processing millions of pixels at once. So, how does that translate into its use in AI? What specific problems are GPUs solving there?\\n\\nDr. Sarah: Exactly, Mike. For AI, especially deep learning and computer vision, youâ€™re often dealing with enormous datasets and performing the same mathematical operations repeatedly across vast amounts of data â€“ like matrix multiplications. A CPU would struggle because it\\'s doing these operations sequentially. A GPU, with its parallel design, can crunch through these calculations incredibly fast. For example, training large language models like GPT-4 wouldn\\'t be feasible without the power of GPUs like NVIDIA\\'s H100 series, which are specifically built with features like Tensor Cores to accelerate these matrix operations.\\n\\nMike: Wow, so itâ€™s less about being \"smarter\" and more about being able to handle a huge volume of the *same kind* of calculations all at once. So, if I\\'m just browsing the web or writing a document, my CPU is doing the heavy lifting. But if I fire up a complex video game or start training an AI model, the GPU kicks in?\\n\\nDr. Sarah: Precisely! For your everyday computing tasksâ€”like browsing, word processing, or managing filesâ€”the CPU is your workhorse. It handles the sequential logic and versatility needed for those operations. But when you move to tasks requiring high-performance parallel computationâ€”be it the complex physics and graphics in a modern video game, or the intensive number-crunching for AI model training or scientific simulationsâ€”the GPU becomes indispensable. Itâ€™s all about choosing the right tool for the specific job.\\n\\nMike: That\\'s a really clear distinction. So, to summarize for our listeners, it\\'s not really a \"CPU vs. GPU\" competition, but more about understanding their strengths and using them synergistically based on the task at hand?\\n\\nDr. Sarah: Absolutely. Thatâ€™s the key takeaway. CPUs are optimized for sequential tasks and overall system management, acting as the brain for general computing. GPUs are optimized for massive parallel processing, excelling in areas like AI, large-scale data processing, and anything that benefits from many operations happening simultaneously. As computing evolves, both continue to advance, complementing each other to power everything we do.\\n\\nMike: Fantastic! That really clarifies it. Dr. Sarah, thank you so much for breaking down CPUs and GPUs for us today.\\n\\nDr. Sarah: My pleasure, Mike!',\n",
              " 'research_podcast.wav')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7HCjQbGuLhB",
        "outputId": "d97ca124-3ac5-4008-9bdf-844f7a262a70"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import asyncio\n",
        "import streamlit as st\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\",temperature=0.2,top_p=0.7)\n",
        "import asyncio\n",
        "from langsmith import traceable\n",
        "from pydantic import BaseModel, Field\n",
        "class Section(BaseModel):\n",
        "    name: str = Field(\n",
        "        description=\"Name for this section of the report.\",\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
        "    )\n",
        "    research: bool = Field(\n",
        "        description=\"Whether to perform web research for this section of the report.\"\n",
        "    )\n",
        "    content: str = Field(\n",
        "        description=\"The content of the section.\"\n",
        "    )\n",
        "\n",
        "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "\n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response['results']\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and 'results' in response:\n",
        "                sources_list.extend(response['results'])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
        "\n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source['url'] not in unique_sources:\n",
        "            unique_sources[source['url']] = source\n",
        "\n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get('raw_content', '')\n",
        "            if raw_content is None:\n",
        "                raw_content = ''\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "\n",
        "    return formatted_text.strip()\n",
        "def format_sections(sections: list[Section]) -> str:\n",
        "    \"\"\" Format a list of sections into a string \"\"\"\n",
        "    formatted_str = \"\"\n",
        "    for idx, section in enumerate(sections, 1):\n",
        "        formatted_str += f\"\"\"\n",
        "          Section {idx}: {section.name}\n",
        "          Description:\n",
        "          {section.description}\n",
        "          Requires Research:\n",
        "          {section.research}\n",
        "\n",
        "          Content:\n",
        "          {section.content if section.content else '[Not yet written]'}\n",
        "\n",
        "          \"\"\"\n",
        "    return formatted_str\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query):\n",
        "    \"\"\" Search the web using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "\n",
        "    Returns:\n",
        "        dict: Tavily search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "\n",
        "    return tavily_client.search(query,\n",
        "                         max_results=5,\n",
        "                         include_raw_content=True)\n",
        "\n",
        "@traceable\n",
        "async def tavily_search_async(search_queries, tavily_topic, tavily_days):\n",
        "    \"\"\"\n",
        "    Performs concurrent web searches using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        search_queries (List[SearchQuery]): List of search queries to process\n",
        "        tavily_topic (str): Type of search to perform ('news' or 'general')\n",
        "        tavily_days (int): Number of days to look back for news articles (only used when tavily_topic='news')\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: List of search results from Tavily API, one per query\n",
        "\n",
        "    Note:\n",
        "        For news searches, each result will include articles from the last `tavily_days` days.\n",
        "        For general searches, the time range is unrestricted.\n",
        "    \"\"\"\n",
        "\n",
        "    search_tasks = []\n",
        "    for query in search_queries:\n",
        "        if tavily_topic == \"news\":\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"news\",\n",
        "                    days=tavily_days\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"general\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Execute all searches concurrently\n",
        "    search_docs = await asyncio.gather(*search_tasks)\n",
        "\n",
        "    return search_docs\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from typing import  Annotated, List, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "class Sections(BaseModel):\n",
        "    sections: List[Section] = Field(\n",
        "        description=\"Sections of the report.\",\n",
        "    )\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(\n",
        "        None, description=\"Query for web search.\"\n",
        "    )\n",
        "class Queries(BaseModel):\n",
        "    queries: List[SearchQuery] = Field(\n",
        "        description=\"List of search queries.\",\n",
        "    )\n",
        "\n",
        "import operator\n",
        "class ReportState(TypedDict):\n",
        "  topic: str\n",
        "  tavily_topic: Literal[\"general\", \"news\"]\n",
        "  tavily_days: Optional[int]\n",
        "  report_structure: str\n",
        "  number_of_queries: int\n",
        "  sections: List[Section]\n",
        "  completed_sections: Annotated[list, operator.add]\n",
        "  report_sections_from_research: str\n",
        "  final_report: str\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "report_planner_query_writer_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "The report will be focused on the following topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report structure will follow these guidelines:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information for planning the report sections.\n",
        "\n",
        "The query should:\n",
        "\n",
        "1. Be related to the topic\n",
        "2. Help satisfy the requirements specified in the report organization\n",
        "\n",
        "Make the query specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\"\"\"\n",
        "\n",
        "report_planner_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "Your goal is to generate the outline of the sections of the report.\n",
        "\n",
        "The overall topic of the report is:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report should follow this organization:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "You should reflect on this information to plan the sections of the report:\n",
        "\n",
        "{context}\n",
        "\n",
        "Now, generate the sections of the report. Each section should have the following fields:\n",
        "\n",
        "- Name - Name for this section of the report.\n",
        "- Description - Brief overview of the main topics and concepts to be covered in this section.\n",
        "- Research - Whether to perform web research for this section of the report.\n",
        "- Content - The content of the section, which you will leave blank for now.\n",
        "\n",
        "Consider which sections require web research. For example, introduction and conclusion will not require research because they will distill information from other parts of the report.\"\"\"\n",
        "\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "\n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "def invoke_structured_llm_with_retry(structured_llm, queries, max_attempts=3):\n",
        "    for _ in range(max_attempts):\n",
        "        results = structured_llm.invoke(queries)\n",
        "        if results:\n",
        "            return results\n",
        "    return results\n",
        "\n",
        "async def generate_report_plan(state: ReportState):\n",
        "\n",
        "    topic = state[\"topic\"]\n",
        "    report_structure = state[\"report_structure\"]\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
        "    results = invoke_structured_llm_with_retry(structured_llm,[SystemMessage(content=system_instructions_query)]+[HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
        "    query_list = [query.search_query for query in results.queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=1000, include_raw_content=True)\n",
        "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str)\n",
        "    structured_llm = llm.with_structured_output(Sections)\n",
        "    report_sections = invoke_structured_llm_with_retry(structured_llm,[SystemMessage(content=system_instructions_sections)]+[HumanMessage(content=\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. Each section must have: name, description, plan, research, and content fields.\")])\n",
        "    return {\"sections\": report_sections.sections}\n",
        "\n",
        "class SectionState(TypedDict):\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    number_of_queries: int # Number web search queries to perform per section\n",
        "    section: Section # Report section\n",
        "    search_queries: list[SearchQuery] # List of search queries\n",
        "    source_str: str # String of formatted source content from web search\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "class SectionOutputState(TypedDict):\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Query writer instructions\n",
        "query_writer_instructions=\"\"\"Your goal is to generate targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "When generating {number_of_queries} search queries, ensure they:\n",
        "1. Cover different aspects of the topic (e.g., core features, real-world applications, technical architecture)\n",
        "2. Include specific technical terms related to the topic\n",
        "3. Target recent information by including year markers where relevant (e.g., \"2024\")\n",
        "4. Look for comparisons or differentiators from similar technologies/approaches\n",
        "5. Search for both official documentation and practical implementation examples\n",
        "\n",
        "Your queries should be:\n",
        "- Specific enough to avoid generic results\n",
        "- Technical enough to capture detailed implementation information\n",
        "- Diverse enough to cover all aspects of the section plan\n",
        "- Focused on authoritative sources (documentation, technical blogs, academic papers)\"\"\"\n",
        "\n",
        "# Section writer instructions\n",
        "section_writer_instructions = \"\"\"You are an expert technical writer crafting one section of a technical report.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "Guidelines for writing:\n",
        "\n",
        "1. Technical Accuracy:\n",
        "- Include specific version numbers\n",
        "- Reference concrete metrics/benchmarks\n",
        "- Cite official documentation\n",
        "- Use technical terminology precisely\n",
        "\n",
        "2. Length and Style:\n",
        "- Strict 150-200 word limit\n",
        "- No marketing language\n",
        "- Technical focus\n",
        "- Write in simple, clear language\n",
        "- Start with your most important insight in **bold**\n",
        "- Use short paragraphs (2-3 sentences max)\n",
        "\n",
        "3. Structure:\n",
        "- Use ## for section title (Markdown format)\n",
        "- Only use ONE structural element IF it helps clarify your point:\n",
        "  * Either a focused table comparing 2-3 key items (using Markdown table syntax)\n",
        "  * Or a short list (3-5 items) using proper Markdown list syntax:\n",
        "    - Use `*` or `-` for unordered lists\n",
        "    - Use `1.` for ordered lists\n",
        "    - Ensure proper indentation and spacing\n",
        "- End with ### Sources that references the below source material formatted as:\n",
        "  * List each source with title, date, and URL\n",
        "  * Format: `- Title : URL`\n",
        "\n",
        "3. Writing Approach:\n",
        "- Include at least one specific example or case study\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- No preamble prior to creating the section content\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Use this source material to help write the section:\n",
        "{context}\n",
        "\n",
        "5. Quality Checks:\n",
        "- Exactly 150-200 words (excluding title and sources)\n",
        "- Careful use of only ONE structural element (table or list) and only if it helps clarify your point\n",
        "- One specific example / case study\n",
        "- Starts with bold insight\n",
        "- No preamble prior to creating the section content\n",
        "- Sources cited at end\"\"\"\n",
        "\n",
        "def generate_queries(state: SectionState):\n",
        "    \"\"\" Generate search queries for a section \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    section = state[\"section\"]\n",
        "\n",
        "    # Generate queries\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = query_writer_instructions.format(section_topic=section.description, number_of_queries=number_of_queries)\n",
        "\n",
        "    # Generate queries\n",
        "    queries = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
        "\n",
        "    return {\"search_queries\": queries.queries}\n",
        "\n",
        "async def search_web(state: SectionState):\n",
        "    \"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n",
        "\n",
        "    # Get state\n",
        "    search_queries = state[\"search_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in search_queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n",
        "\n",
        "    return {\"source_str\": source_str}\n",
        "\n",
        "def write_section(state: SectionState):\n",
        "    \"\"\" Write a section of the report \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    section = state[\"section\"]\n",
        "    source_str = state[\"source_str\"]\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=source_str)\n",
        "\n",
        "    # Generate section\n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "\n",
        "    # Write content to the section object\n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "# Add nodes and edges\n",
        "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
        "section_builder.add_node(\"generate_queries\", generate_queries)\n",
        "section_builder.add_node(\"search_web\", search_web)\n",
        "section_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "section_builder.add_edge(START, \"generate_queries\")\n",
        "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
        "section_builder.add_edge(\"search_web\", \"write_section\")\n",
        "section_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Compile\n",
        "section_builder_graph = section_builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(section_builder_graph.get_graph(xray=1).draw_mermaid_png()))\n",
        "class ReportStateOutput(TypedDict):\n",
        "    final_report: str # Final report\n",
        "\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "\n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "# Tavily search parameters\n",
        "tavily_topic = \"general\"\n",
        "tavily_days = None # Only applicable for news topic\n",
        "\n",
        "from langgraph.constants import Send\n",
        "\n",
        "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
        "\n",
        "Section to write:\n",
        "{section_topic}\n",
        "\n",
        "Available report content:\n",
        "{context}\n",
        "\n",
        "1. Section-Specific Approach:\n",
        "\n",
        "For Introduction:\n",
        "- Use # for report title (Markdown format)\n",
        "- 50-100 word limit\n",
        "- Write in simple and clear language\n",
        "- Focus on the core motivation for the report in 1-2 paragraphs\n",
        "- Use a clear narrative arc to introduce the report\n",
        "- Include NO structural elements (no lists or tables)\n",
        "- No sources section needed\n",
        "\n",
        "For Conclusion/Summary:\n",
        "- Use ## for section title (Markdown format)\n",
        "- 100-150 word limit\n",
        "- For comparative reports:\n",
        "    * Must include a focused comparison table using Markdown table syntax\n",
        "    * Table should distill insights from the report\n",
        "    * Keep table entries clear and concise\n",
        "- For non-comparative reports:\n",
        "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
        "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
        "    * Or a short list using proper Markdown list syntax:\n",
        "      - Use `*` or `-` for unordered lists\n",
        "      - Use `1.` for ordered lists\n",
        "      - Ensure proper indentation and spacing\n",
        "- End with specific next steps or implications\n",
        "- No sources section needed\n",
        "\n",
        "3. Writing Approach:\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Quality Checks:\n",
        "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
        "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
        "- Markdown format\n",
        "- Do not include word count or any preamble in your response\"\"\"\n",
        "\n",
        "def initiate_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"\n",
        "\n",
        "    # Kick off section writing in parallel via Send() API for any sections that require research\n",
        "    return [\n",
        "        Send(\"build_section_with_web_research\", {\"section\": s,\n",
        "                                                 \"number_of_queries\": state[\"number_of_queries\"],\n",
        "                                                 \"tavily_topic\": state[\"tavily_topic\"],\n",
        "                                                 \"tavily_days\": state.get(\"tavily_days\", None)})\n",
        "        for s in state[\"sections\"]\n",
        "        if s.research\n",
        "    ]\n",
        "\n",
        "def write_final_sections(state: SectionState):\n",
        "    \"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n",
        "\n",
        "    # Get state\n",
        "    section = state[\"section\"]\n",
        "    completed_report_sections = state[\"report_sections_from_research\"]\n",
        "\n",
        "    system_instructions = final_section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=completed_report_sections)\n",
        "\n",
        "    # Generate section\n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    section.content = section_content.content\n",
        "\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "def gather_completed_sections(state: ReportState):\n",
        "    \"\"\" Gather completed sections from research \"\"\"\n",
        "\n",
        "    # List of completed sections\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    # Format completed section to str to use as context for final sections\n",
        "    completed_report_sections = format_sections(completed_sections)\n",
        "\n",
        "    return {\"report_sections_from_research\": completed_report_sections}\n",
        "\n",
        "def initiate_final_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off research on any sections that require it using the Send API \"\"\"\n",
        "\n",
        "    # for any sections that do not require research\n",
        "    return [\n",
        "        Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]})\n",
        "        for s in state[\"sections\"]\n",
        "        if not s.research\n",
        "    ]\n",
        "\n",
        "def compile_final_report(state: ReportState):\n",
        "    \"\"\" Compile the final report \"\"\"\n",
        "\n",
        "    # Get sections\n",
        "    sections = state[\"sections\"]\n",
        "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
        "\n",
        "    # Update sections with completed content while maintaining original order\n",
        "    for section in sections:\n",
        "        section.content = completed_sections[section.name]\n",
        "\n",
        "    # Compile final report\n",
        "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
        "\n",
        "    return {\"final_report\": all_sections}\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(ReportState, output=ReportStateOutput)\n",
        "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
        "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
        "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
        "builder.add_node(\"write_final_sections\", write_final_sections)\n",
        "builder.add_node(\"compile_final_report\", compile_final_report)\n",
        "builder.add_edge(START, \"generate_report_plan\")\n",
        "builder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\n",
        "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
        "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
        "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
        "builder.add_edge(\"compile_final_report\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import wave\n",
        "\n",
        "# Set up the wave file to save the output:\n",
        "def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
        "   with wave.open(filename, \"wb\") as wf:\n",
        "      wf.setnchannels(channels)\n",
        "      wf.setsampwidth(sample_width)\n",
        "      wf.setframerate(rate)\n",
        "      wf.writeframes(pcm)\n",
        "def create_podcast_discussion(final_report, topic, filename=\"research_podcast.wav\", configuration=None):\n",
        "      script_prompt = f\"\"\"\n",
        "    Create a natural, engaging podcast conversation between Dr. Sarah (research expert) and Mike (curious interviewer) about \"{topic}\".\n",
        "\n",
        "    Use this research content:\n",
        "\n",
        "  FINAL REPORT GENERATED BASED ON THE WEBSEARCH AND RESEARCH RESULTS:\n",
        "  {final_report}\n",
        "\n",
        "    Format as a dialogue with:\n",
        "    - Mike introducing the topic and asking questions\n",
        "    - Dr. Sarah explaining key concepts and insights\n",
        "    - Natural back-and-forth discussion (5-7 exchanges)\n",
        "    - Mike asking follow-up questions\n",
        "    - Dr. Sarah synthesizing the main takeaways\n",
        "    - Keep it conversational and accessible (2 or 3-4 minutes when spoken)\n",
        "\n",
        "    Format exactly like this:\n",
        "    Mike: [opening question]\n",
        "    Dr. Sarah: [expert response]\n",
        "    Mike: [follow-up]\n",
        "    Dr. Sarah: [explanation]\n",
        "    [continue...]\n",
        "    \"\"\"\n",
        "      script_response = client.models.generate_content(\n",
        "          model=\"gemini-2.5-flash\",\n",
        "          contents=script_prompt\n",
        "      )\n",
        "      podcast_script = script_response.candidates[0].content.parts[0].text\n",
        "      tts_prompt = f\"TTS the following conversation between Mike and Dr. Sarah:\\n{podcast_script}\"\n",
        "\n",
        "      # Use default values if configuration is None\n",
        "      tts_model = getattr(configuration, 'tts_model', \"gemini-2.5-flash-preview-tts\")\n",
        "      mike_voice = getattr(configuration, 'mike_voice', 'Kore')\n",
        "      sarah_voice = getattr(configuration, 'sarah_voice', 'Puck')\n",
        "      tts_channels = getattr(configuration, 'tts_channels', 1)\n",
        "      tts_rate = getattr(configuration, 'tts_rate', 24000)\n",
        "      tts_sample_width = getattr(configuration, 'tts_sample_width', 2)\n",
        "\n",
        "\n",
        "      response = client.models.generate_content(\n",
        "          model=tts_model,\n",
        "          contents=tts_prompt,\n",
        "          config=types.GenerateContentConfig(\n",
        "              response_modalities=[\"AUDIO\"],\n",
        "              speech_config=types.SpeechConfig(\n",
        "                  multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n",
        "                      speaker_voice_configs=[\n",
        "                          types.SpeakerVoiceConfig(\n",
        "                              speaker='Mike',\n",
        "                              voice_config=types.VoiceConfig(\n",
        "                                  prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                      voice_name=mike_voice,\n",
        "                                  )\n",
        "                              )\n",
        "                          ),\n",
        "                          types.SpeakerVoiceConfig(\n",
        "                              speaker='Dr. Sarah',\n",
        "                              voice_config=types.VoiceConfig(\n",
        "                                  prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                      voice_name=sarah_voice,\n",
        "                                  )\n",
        "                                )\n",
        "                            ),\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "      audio_data = response.candidates[0].content.parts[0].inline_data.data\n",
        "      wave_file(filename, audio_data, tts_channels, tts_rate, tts_sample_width)\n",
        "\n",
        "      print(f\"Podcast saved as: {filename}\")\n",
        "      return podcast_script, filename\n",
        "\n"
      ],
      "metadata": {
        "id": "t3rG0fpGSR8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa154188-ac6e-448b-eab7-1a7bfd8dca44"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py\n",
        "import asyncio\n",
        "from app import report_structure, tavily_topic, tavily_days, graph, create_podcast_discussion\n",
        "\n",
        "async def generate_report_and_podcast(topic, num_queries):\n",
        "    report = await graph.ainvoke({\n",
        "        \"topic\": topic,\n",
        "        \"report_structure\": report_structure,\n",
        "        \"number_of_queries\": num_queries,\n",
        "        \"tavily_topic\": tavily_topic,\n",
        "        \"tavily_days\": tavily_days\n",
        "    })\n",
        "\n",
        "    podcast_script, podcast_filename = create_podcast_discussion(report[\"final_report\"], topic)\n",
        "    return report, podcast_script, podcast_filename\n",
        "\n",
        "st.title(\"ðŸ“„ AI-Powered Report and Podcast Generator\")\n",
        "st.write(\n",
        "    \"\"\"\n",
        "    Enter a topic below and hit **Generate**.\n",
        "    The AI will fetch data, structure it according to your notebookâ€™s\n",
        "    `report_structure`, and display the final report.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "topic = st.text_input(\"ðŸ” Topic\", placeholder=\"e.g. â€œDeep learning on edge devicesâ€\")\n",
        "num_queries = st.slider(\"Number of AI queries (controls depth)\", 1, 5, 2)\n",
        "\n",
        "if st.button(\"Generate Report\") and topic.strip():\n",
        "    with st.spinner(\"Generating report and podcastâ€¦\"):\n",
        "        report, podcast_script, podcast_filename = asyncio.run(generate_report_and_podcast(topic, num_queries))\n",
        "\n",
        "    st.subheader(\"Podcast Discussion\")\n",
        "    st.audio(podcast_filename)\n",
        "\n",
        "    # Add download link for the podcast\n",
        "    with open(podcast_filename, \"rb\") as f:\n",
        "        st.download_button(\n",
        "            label=\"Download Podcast\",\n",
        "            data=f,\n",
        "            file_name=podcast_filename,\n",
        "            mime=\"audio/wav\"\n",
        "        )\n",
        "\n",
        "    st.subheader(\"Podcast Script\")\n",
        "    st.text_area(\"Script\", podcast_script, height=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi4BBOC1fibx",
        "outputId": "ea0bbfa7-1771-498d-aafe-462461123377"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -c https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-v3-stable-linux-amd64.zip\n",
        "!chmod +x ngrok"
      ],
      "metadata": {
        "id": "hiU2O5LNuyUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9ab635-5c56-452f-bae9-68928ae4b7db"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-v3-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok config add-authtoken 2q9gHoz4I2P9TK9966QZ5snqfJ7_6gb71suopfXZf1Q3cENHe"
      ],
      "metadata": {
        "id": "-ITYejOEvFP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120e71ab-d372-457b-ed5d-c5519ab312fe"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "BErP-ix5vHxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3930cc14-738e-44cf-a8e1-cf2e2aa87b63"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill all tunnels from previous sessions\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "Z8X_qSKEgkBG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Open the app here ðŸ‘‰\", public_url)\n",
        "\n",
        "# Relaunch Streamlit app\n",
        "!streamlit run app1.py &> /dev/null &"
      ],
      "metadata": {
        "id": "FIQZqamivL3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf8c690-78a2-4c72-e305-af0c4b8faf41"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open the app here ðŸ‘‰ NgrokTunnel: \"https://c28518dadbda.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}